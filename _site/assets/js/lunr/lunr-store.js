var store = [{
        "title": "Exploring Big Data",
        "excerpt":"The topics traversed here are a combination of academic areas explored while studying at Rutgers as well as areas I found of interest since drinking the big data kool-aid. If you find areas for improving this content or are looking for more detailed understanding of the subject areas send me an email. ","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/BigData/",
        "teaser":"http://0.0.0.0:4000/assets/images/my-awesome-post-teaser.jpg"},{
        "title": "Data Lakes: Overview",
        "excerpt":"Oxford Botanic Garden Pond Part 1 Notes From Concept to Implementation This is the first blog under the topic Data Lakes and will be followed by additional topics pertaining to data lakes.  Each post will start with the title Data Lakes followed by a [:new topic].  Each topic will go deeper and share additional details and links to practical tools that I have discovered.  The goal is to better understand the impact of data lakes on the analytics value chain.   This post broadly covers the following questions:   Where did the data lake concept originate?  What are the problems that data lakes are solving or not solving?  What are the typical components of enterprise data lakes?  How are data lakes being used to increase productivity or share data?  What are the data governance issues that need to be addressed?1. **Where did the data lake concept originate? “Google, Facebook and Yahoo had discovered big data before anyone else, but it was obvious that it was coming to banks, hospitals and other traditional companies. Enterprises had already realized that cloud computing was important. They were rebuilding their data centers to be cloudy. We realized that a once‑in‑a‑generation technology change was underway, and knew that we had a chance to create a company to deliver the modern platform for big data”  Cloudera.  While not inventing the term, Cloudera embraced the Apache open source projects to build a data management platform that could scale the way of Google, Facebook and Yahoo. Prior to data lakes, technologies were architected and created to solve specific data related problems.  These special purpose legacy architectures were expensive and inflexible.  Examples of these systems included relational databases for back-office reporting and billing, to large document management systems for storing unstructured data.  Those companies born from the web were the first to encounter the limitations of processing large volumes of data with data of different variety.  Building on the idea’s of  Google, Apache Hadoop software was created by Doug Cutting and Mike Carfarella.  Hadoop was comprised of HDFS and MapReduce technologies. The primary use of a data lake is to store data.  The Apache Hadoop project is primarily spoken of as the source for storing data and has become the default term by most data professionals when referring to data lakes. 2. **What are the problems that data lakes are solving or not solving? Data lakes are perceived very much in the same light as the original data warehouse. The promise of answering important strategic as well as predicting success or failure based on large volumes of data have been the primary reason to spend. Predicting the future is not easy. Many of the data science or predictive models have not had as high returns as originally thought.  More successful approaches to implementing data lakes have started with a small single cross departmental data problem that can show a high ROI.  A project such as reducing customer churn that reaches across two or three organizations including support, product analytics, sales and a renewals team could be one example.  Another project could be one related to a central point of governance.  This could be a data lake to create a centralized entity resolution dataset to monitor and manage sensitive data. The transforming of data, or the T in ETL (extracting, transforming, and loading) is the process where Hadoop has been thought to cause the greatest improvements.  Because much of the transforming of data in the process is done on an ETL server, a Hadoop cluster architecture is much more efficient at transforming the data as it gets loaded to the data lake.  Additionally, data that is not in tabular form can be saved in its native form and avoid converting the data to a target schema.   This is especially important to vast volumes of unstructured IOT data. Some of the other benefits include allowing for non-programmers to create data visualizations, reports, dashboard and SQL like access to provide for analysis. One of the pitfalls to Data lakes has been couched in the “data puddles” term.  These stranded data lakes become a problem when the proliferation of data lakes leads to data quality and cost problems of implementing duplicative systems.  Data puddles are meant to be avoided with the proper implementation by IT versus separate business initiatives. 3.  **What are the typical components of enterprise data lakes? Hadoop is a massively parallel storage and execution platform.  Hadoop includes a distributed files system (HDFS) that some vendors like MapR and IBM provide their own proprietary files systems. Hadoop is a platform and an ecosystem that incorporates many of the most used open source projects including Hive and Spark.  Hive allows for SQL like interface to Hadoop and Spark turns Hadoop to an in-memory execution system.  Hadoop provides for an extremely scalable cost effective system that is modular in form.  The system has a schema-on-read architecture unlike relational databases.  The schema-on-read nature of Hadoop allows for data to be added quickly to the data lake without needing to know the structure of the data to write to disk.  This avoids the high cost of processing the data when it is being collected and written to the store. Many of the same issues regarding data quality continue to be components that must be addressed with data lakes.  Data completeness,  data type (string, number..) format, cardinality (unique values) and referential integrity are rules that still need to be considered when designing and implementing data lakes. One added concept to data lakes is lineage.  Lineage accounts for setting priorities based on where the data was derived.  Understanding the source of the data was from a spreadsheet or from a system of record will be key to understanding which data has priority.  Implementing lineage can be difficult as multiple tools could be used to access the same dataset without the data lake understanding the same data is being added. There are some that would argue that one ETL tool could provide a more rigorous approach to centralizing the collection process. Granularity and transformation of data are two concepts that a data lake needs to consider to create reliable data sets.  Within granularity, data set level refers to the  directed graph representation of relationship between datasets while field level lineage is also represented between fields. The transformational representation needs to be translated to a common representation where the lineage of the data needs to be interpreted in a manner that can be more easily understood despite all changes that have occurred.  Read more about the process whereby the focus on denormalization becomes critical in the context of the data lake and specifically in terms of any associated Hadoop/HDFS data structures.  Above is an example of a generic data lake architecture from Alex Gorelik’s book The Enterprise Big Data Lake.The raw or landing zone within the data lake context is usually where files are loaded from external systems.  The gold area is where it is transformed data, while the work folder with data is where users projects and data is stored.  The sensitive data area is where the encrypted files are stored. Given the size of the data lake and lack of a search function, finding relevant data is extremely cumbersome in a data lake without a catalog.  Although Hue, the Hadoop utility, allows for some limited preview of data files additional tools from vendors like Teradata’s Loom  and Waterline Data Inventory are used to catalog Hadoop data besides relational databases to run outside of Hadoop for the cataloging data. Additional cataloging tools are available from Cloudera’s Navigator product.  One open source project, Apache Atlas, has been created to help solve the meta data and governance framework for Hadoop. 4.  **How are data lakes being used to increase productivity or share data? Besides the technical requirements, some organizations are struggling to implement Hadoop as there is a battle between the silos of the business user and IT for control of the Hadoop cluster.   This is where the democratization of data is one of the first steps to capitalizing on a companies digital assets. Like markets, the potential of data is curtailed when data is not transparent.  Early in my transition to using corporate data, I discovered the only way to perform any meaningful analysis was to ask questions of the available data. I quickly realized that unless I was querying the data myself, the data was either out of date or incomplete. I subsequently took my first database class at the local community college.  New self-service tools such as Tableau and Qlik are allowing power users or subject matter experts to create and transform data to allow business users to more quickly access the data to help answer business questions. The problem with the traditional business analyst process of the past is the data needed to be found, provisioned, prepped, and then analyzed.  Part of the process included creating critical data elements can not rapidly change as according to new user requirements . This is a complete contrast to growing up with Google and Wikipedia.  The pure volume of data makes it impossible to document and tag everything, this is where new tools that leverage machine learning and auto-tracking of data consumption creates catalogs of tribal knowledge.  This is much a kin to the search scoring that powers search engines. In order to make consumption of the data easier, many tools are being used to prepare the data to be consumed applications.  This is where the data wrangling aspect of most data mining and data science projects spend the majority of the time. Like Excel, which is impractical for large data lake files, newer tools are being used to clean and shape the data to the right form.  Some of the vendors providing these tools are Alteryx, Datameer, Paxata, and Trifacta as well as traditional vendors such as Informatica and Talend provide tools with the idea of providing self-service for the business analyst. Combining data or joining data sets is where much of the value from data lakes becomes immediately useful.  Unfortunately, common ids are uncommon amongst separate data sets.  This is where the idea of entity resolution becomes instrumental in capturing value from the data.  Master Data Management systems with probabilistic matching engines that have rules for defining matched entities help in joining data sets.  One company providing entity resolution products is Tamr. 5.  **What are the data governance issues that need to be addressed? The centralized nature of a data lake could allow for greater governance versus heterogenous systems that use different technologies.  Stewardship of data lakes is key to creating trusted users that have organizational responsibility for the data.  Some form of provisioning of the data is one of the critical problems of data lakes.  One approach that has been widely adopted is only allowing analysts access to data they need for there specific job function.  An example could be allowing access based on location or some other form of limiting data. Data lake users and zones are typically segmented into four zones: a landing zone where data scientists, developers, and data engineers have access; a gold zone where analysts and data scientists have access; a work zone for data scientists and data engineers; and finally a sensitive zone where data stewards and trusted analysts have access. One approach to governance is to implement a method for de-identifying sensitive data.  Since the data is written quickly without schema, the ability to tag data on original write is limited for efficiency reasons.  Some profiling tools have been built to scan the data lake and look for sensitive data.  These tools include DataGuise and Waterline Data. Another approach is transparent encryption that is offered by Cloudera Navigator  that encrypts data when it is written and de-crypts the data when read. Credits Icons + Demo Images:   The Noun Project – Garrett Knoll, Arthur Shlain, and tracy tam  Font Awesome  UnsplashOther:   Jekyll  jQuery  Susy  Breakpoint  Magnific Popup  FitVids.JS  Greedy Navigation - lukejacksonn  jQuery Smooth Scroll  LunrMinimal Mistakes is designed, developed, and maintained by Michael Rose. Just another boring, tattooed, designer from Buffalo New York. ","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/datalakes/",
        "teaser":"http://0.0.0.0:4000/assets/images/my-awesome-post-teaser.jpg"},{
        "title": "Entity Resolution: Overview",
        "excerpt":"What is Entity Resolution? Entity Resolution is the task of disambiguating manifestations of real world entities in various records or mentions by linking and grouping.1 Effectively reducing something to its most basic conical form. Linking entities to form relationships and draw conclusions from data is becoming increasingly relevant as data lakes become prevalent in organizations and protecting data integrity expectations increases especially as it relates to making use of private personal data. Many organizations ranging from government and public health data, web search, comparison shopping, law enforcement are being expected to provide a higher level of transparency and protection of the data being collected and used for predicting behavior and accomplishing their goals. References             website:www.datacommunitydc.org &#8617;       ","categories": ["Exploring Big Data"],
        "tags": [],
        "url": "http://0.0.0.0:4000/entityresolution_over/",
        "teaser":"http://0.0.0.0:4000/assets/images/my-awesome-post-teaser.jpg"},{
        "title": "Identity Resolution",
        "excerpt":"Exploring different models to eliminate redundant and erroneous text records from different sources to manage personally identifiable information(PII) Overview Identify and link the Same Person Complying with privacy and information security laws is increasingly becoming more challenging for individuals and organizations.  Events such as the 2017 Equifax data breach of 143 million individual identities is becoming a more common occurrence.  Even more concerning, in 2017, Yahoo revised its 2013 500 million Yahoo user identity breach to 3 billion identities being stolen.  The scale of these security breaches impact on an organization’s perceived business value and the direct negative financial implications from lax regulatory adherence should not be underestimated.1 According to wikipedia, “personally identifiable information(PII) or sensitive personal information as used in information security and privacy laws…has four common variants based on personal | personally, and identifiable | identifying”  information.  Protecting personal information is governed according to privacy principles in most OECD countries.  In Europe, data protection laws are mainly focused on General Data Protection Regulation(GDPR).Depending on jurisdiction and differing regulatory pressures, an organizations abilities to comply with protecting PII starts with managing data assets effectively and promoting a culture that values privacy and digital policies.  One of the most challenging aspects of complying with regulatory or managing reputational risks associated with keeping PII information secure is an organization’s ability to maintain information governance standards that effectively manages personal identifiable information.2 One such approach to managing PII data more effectively is for organizations to identify and link persons across all its data assets into one consolidated entity resolution index.  Basic information governance will result from combining datasets and performing large aggregate analyses using powerful new ways to improve service across large populations. Critically important in this task is the deduplication of identities across multiple data sets that were rarely designed to work together. Inconsistent data entry, typographical errors, and real world identity changes pose significant challenges to this process. Besides adhering to PII standards, the ability to correlate personal identities from disparate data sources allows approximate joins to occur giving increased value to the joined datasets.  One could consider the Department of Homeland Security attempt at joining passenger data with homeland security no-fly lists and the and other potential terrorism security measures an extreme in linking related personal identities. Description of Concepts Learned “Entity resolution is to distinguish the representations referring to the same real world object entity in one or more databases and recognize all different real-world entities in the databases.”3 According to Wang there are two main types of classifications of entity resolution based on pairwise entity resolution and group-wise entity resolution.  As an example, for entity resolution on name set {o1=“Wei Wang”, o2=“W Wei”, o3=“Wang Wei”, o4=“Jian Pei”} the result of pair-wise entity resolution is {(o1, o2), (o1, o3), (o2, o3)} The  pairwise resolution refers to a pair of data objects referring to the same real-world entity while the group-wire resolution refers to a family clusters with each data object referring to the same real-world entity.   The challenges of entity resolution relate to the size of data, the frequency of data updates, the structure of the data, and the accuracy of entity resolution.  For the scope of this project, the data quality management aspect of resolving the same entity in different databases will be explored.  The main concept to be learned will be to find duplicate tuples as they relate to a firm’s individual contact information.  The ability to clean dirty data by detecting the objects referring to the same real world entity will result in a consolidated entity based management approach. Creating a single source for entity management will allow more effective maintenance and deletion of such entities from the systems containing personally identifiable information. While Wang’s approach to entity resolution is more a kin to using similarity computations, Mikhail Bilenko’s approach describes “Learnable Similarity Functions”4 as the best means to reducing entities to there most basic form. Entity resolution on names and context-based entity resolution will be explored. Time permitting, further research in regard to the ER problem as widely used in many domains such as natural language processing (NLP), machine learning and databases will be explored. In short, the models explored will explore the most accurate methods for merging customer lists using personal identity variables to avoid redundancy and allow for a master list for more efficient management of personal identities. References             Armerding, Taylor. “The 16 Biggest Data Breaches of the 21st Century.” CSO Online. CSO, 11 Oct. 2017. Web. 21 Jan. 2018. &#8617;               “Personally Identifiable Information.” Wikipedia. Wikimedia Foundation, 19 Jan. 2018. Web. 21 Jan. 2018. &#8617;               Wang, Hongzhi. “Overview of Entity Resolution.” Innovative Techniques and Applications of Entity Resolution. Hershey, PA: Information Science Reference/IGI Global, 2014. N. pag. Print. &#8617;               Bilenko, Mikhail. “Learnable Similarity Functions and Their Applications to Clustering and Record Linkage.” Proceedings of the Ninth AAAI/SIGART Doctoral Consortium, pp. 981-982, San Jose, CA, July 2004 &#8617;       ","categories": ["Exploring Big Data"],
        "tags": [],
        "url": "http://0.0.0.0:4000/entityresolution/",
        "teaser":"http://0.0.0.0:4000/assets/images/my-awesome-post-teaser.jpg"},{
        "title": "Data Lakes: Overview- in posts",
        "excerpt":"Oxford Botanic Garden Pond Part 1 Notes From Concept to Implementation This is the first blog under the topic Data Lakes and will be followed by additional topics pertaining to data lakes.  Each post will start with the title Data Lakes followed by a [:new topic].  Each topic will go deeper and share additional details and links to practical tools that I have discovered.  The goal is to better understand the impact of data lakes on the analytics value chain.   This post broadly covers the following questions:   Where did the data lake concept originate?  What are the problems that data lakes are solving or not solving?  What are the typical components of enterprise data lakes?  How are data lakes being used to increase productivity or share data?  What are the data governance issues that need to be addressed?1. **Where did the data lake concept originate? “Google, Facebook and Yahoo had discovered big data before anyone else, but it was obvious that it was coming to banks, hospitals and other traditional companies. Enterprises had already realized that cloud computing was important. They were rebuilding their data centers to be cloudy. We realized that a once‑in‑a‑generation technology change was underway, and knew that we had a chance to create a company to deliver the modern platform for big data”  Cloudera.  While not inventing the term, Cloudera embraced the Apache open source projects to build a data management platform that could scale the way of Google, Facebook and Yahoo. Prior to data lakes, technologies were architected and created to solve specific data related problems.  These special purpose legacy architectures were expensive and inflexible.  Examples of these systems included relational databases for back-office reporting and billing, to large document management systems for storing unstructured data.  Those companies born from the web were the first to encounter the limitations of processing large volumes of data with data of different variety.  Building on the idea’s of  Google, Apache Hadoop software was created by Doug Cutting and Mike Carfarella.  Hadoop was comprised of HDFS and MapReduce technologies. The primary use of a data lake is to store data.  The Apache Hadoop project is primarily spoken of as the source for storing data and has become the default term by most data professionals when referring to data lakes. 2. **What are the problems that data lakes are solving or not solving? Data lakes are perceived very much in the same light as the original data warehouse. The promise of answering important strategic as well as predicting success or failure based on large volumes of data have been the primary reason to spend. Predicting the future is not easy. Many of the data science or predictive models have not had as high returns as originally thought.  More successful approaches to implementing data lakes have started with a small single cross departmental data problem that can show a high ROI.  A project such as reducing customer churn that reaches across two or three organizations including support, product analytics, sales and a renewals team could be one example.  Another project could be one related to a central point of governance.  This could be a data lake to create a centralized entity resolution dataset to monitor and manage sensitive data. The transforming of data, or the T in ETL (extracting, transforming, and loading) is the process where Hadoop has been thought to cause the greatest improvements.  Because much of the transforming of data in the process is done on an ETL server, a Hadoop cluster architecture is much more efficient at transforming the data as it gets loaded to the data lake.  Additionally, data that is not in tabular form can be saved in its native form and avoid converting the data to a target schema.   This is especially important to vast volumes of unstructured IOT data. Some of the other benefits include allowing for non-programmers to create data visualizations, reports, dashboard and SQL like access to provide for analysis. One of the pitfalls to Data lakes has been couched in the “data puddles” term.  These stranded data lakes become a problem when the proliferation of data lakes leads to data quality and cost problems of implementing duplicative systems.  Data puddles are meant to be avoided with the proper implementation by IT versus separate business initiatives. 3.  **What are the typical components of enterprise data lakes? Hadoop is a massively parallel storage and execution platform.  Hadoop includes a distributed files system (HDFS) that some vendors like MapR and IBM provide their own proprietary files systems. Hadoop is a platform and an ecosystem that incorporates many of the most used open source projects including Hive and Spark.  Hive allows for SQL like interface to Hadoop and Spark turns Hadoop to an in-memory execution system.  Hadoop provides for an extremely scalable cost effective system that is modular in form.  The system has a schema-on-read architecture unlike relational databases.  The schema-on-read nature of Hadoop allows for data to be added quickly to the data lake without needing to know the structure of the data to write to disk.  This avoids the high cost of processing the data when it is being collected and written to the store. Many of the same issues regarding data quality continue to be components that must be addressed with data lakes.  Data completeness,  data type (string, number..) format, cardinality (unique values) and referential integrity are rules that still need to be considered when designing and implementing data lakes. One added concept to data lakes is lineage.  Lineage accounts for setting priorities based on where the data was derived.  Understanding the source of the data was from a spreadsheet or from a system of record will be key to understanding which data has priority.  Implementing lineage can be difficult as multiple tools could be used to access the same dataset without the data lake understanding the same data is being added. There are some that would argue that one ETL tool could provide a more rigorous approach to centralizing the collection process. Granularity and transformation of data are two concepts that a data lake needs to consider to create reliable data sets.  Within granularity, data set level refers to the  directed graph representation of relationship between datasets while field level lineage is also represented between fields. The transformational representation needs to be translated to a common representation where the lineage of the data needs to be interpreted in a manner that can be more easily understood despite all changes that have occurred.  Read more about the process whereby the focus on denormalization becomes critical in the context of the data lake and specifically in terms of any associated Hadoop/HDFS data structures.  Above is an example of a generic data lake architecture from Alex Gorelik’s book The Enterprise Big Data Lake.The raw or landing zone within the data lake context is usually where files are loaded from external systems.  The gold area is where it is transformed data, while the work folder with data is where users projects and data is stored.  The sensitive data area is where the encrypted files are stored. Given the size of the data lake and lack of a search function, finding relevant data is extremely cumbersome in a data lake without a catalog.  Although Hue, the Hadoop utility, allows for some limited preview of data files additional tools from vendors like Teradata’s Loom  and Waterline Data Inventory are used to catalog Hadoop data besides relational databases to run outside of Hadoop for the cataloging data. Additional cataloging tools are available from Cloudera’s Navigator product.  One open source project, Apache Atlas, has been created to help solve the meta data and governance framework for Hadoop. 4.  **How are data lakes being used to increase productivity or share data? Besides the technical requirements, some organizations are struggling to implement Hadoop as there is a battle between the silos of the business user and IT for control of the Hadoop cluster.   This is where the democratization of data is one of the first steps to capitalizing on a companies digital assets. Like markets, the potential of data is curtailed when data is not transparent.  Early in my transition to using corporate data, I discovered the only way to perform any meaningful analysis was to ask questions of the available data. I quickly realized that unless I was querying the data myself, the data was either out of date or incomplete. I subsequently took my first database class at the local community college.  New self-service tools such as Tableau and Qlik are allowing power users or subject matter experts to create and transform data to allow business users to more quickly access the data to help answer business questions. The problem with the traditional business analyst process of the past is the data needed to be found, provisioned, prepped, and then analyzed.  Part of the process included creating critical data elements can not rapidly change as according to new user requirements . This is a complete contrast to growing up with Google and Wikipedia.  The pure volume of data makes it impossible to document and tag everything, this is where new tools that leverage machine learning and auto-tracking of data consumption creates catalogs of tribal knowledge.  This is much a kin to the search scoring that powers search engines. In order to make consumption of the data easier, many tools are being used to prepare the data to be consumed applications.  This is where the data wrangling aspect of most data mining and data science projects spend the majority of the time. Like Excel, which is impractical for large data lake files, newer tools are being used to clean and shape the data to the right form.  Some of the vendors providing these tools are Alteryx, Datameer, Paxata, and Trifacta as well as traditional vendors such as Informatica and Talend provide tools with the idea of providing self-service for the business analyst. Combining data or joining data sets is where much of the value from data lakes becomes immediately useful.  Unfortunately, common ids are uncommon amongst separate data sets.  This is where the idea of entity resolution becomes instrumental in capturing value from the data.  Master Data Management systems with probabilistic matching engines that have rules for defining matched entities help in joining data sets.  One company providing entity resolution products is Tamr. 5.  **What are the data governance issues that need to be addressed? The centralized nature of a data lake could allow for greater governance versus heterogenous systems that use different technologies.  Stewardship of data lakes is key to creating trusted users that have organizational responsibility for the data.  Some form of provisioning of the data is one of the critical problems of data lakes.  One approach that has been widely adopted is only allowing analysts access to data they need for there specific job function.  An example could be allowing access based on location or some other form of limiting data. Data lake users and zones are typically segmented into four zones: a landing zone where data scientists, developers, and data engineers have access; a gold zone where analysts and data scientists have access; a work zone for data scientists and data engineers; and finally a sensitive zone where data stewards and trusted analysts have access. One approach to governance is to implement a method for de-identifying sensitive data.  Since the data is written quickly without schema, the ability to tag data on original write is limited for efficiency reasons.  Some profiling tools have been built to scan the data lake and look for sensitive data.  These tools include DataGuise and Waterline Data. Another approach is transparent encryption that is offered by Cloudera Navigator  that encrypts data when it is written and de-crypts the data when read. ","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Datalake1/",
        "teaser":"http://0.0.0.0:4000/assets/images/my-awesome-post-teaser.jpg"}]
